{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "# model_name = 'qanastek/51-languages-classifier'\n",
        "model_name = 'huggingface/CodeBERTa-small-v1'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1704451218036
        }
      },
      "id": "0d420eba-2def-41b2-b978-5ef5df58b397"
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
        "# res = classifier(\"hello how are you ?\")\n",
        "# print(res)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1704451218199
        }
      },
      "id": "4771aa96-f2a4-4165-b257-abaf9ff12acf"
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(\"qanastek/51-languages-classifier\")\n",
        "\n",
        "# code_sample = \"i am maninder')\"\n",
        "\n",
        "# tokens = tokenizer(code_sample, return_tensors=\"pt\")\n",
        "# print(tokens)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1704451218347
        }
      },
      "id": "8c525d34-9a3e-48fd-b4a8-4172bf236306"
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "data = datasets.load_from_disk('preprocessed_programming_language_data_count_great500')\n",
        "data = data.shuffle(seed= 42)\n",
        "# data = data.train_test_split(0.1)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1704451220081
        }
      },
      "id": "876baca6-3f83-4045-a305-64c6b6b1574d"
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "Dataset({\n    features: ['code', 'language_name'],\n    num_rows: 66148\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1704451220405
        }
      },
      "id": "07cd8abf-4968-4013-8ca5-01104fd5e8cd"
    },
    {
      "cell_type": "code",
      "source": [
        "# data = datasets.load_dataset('pandas','preprocessed_programming_language_data.csv')\n",
        "data = data.rename_column('code','text')\n",
        "data = data.rename_column('language_name','label')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1704451220549
        }
      },
      "id": "7d856fb6-61cc-4baf-87a6-eb03ffc4cdb1"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = data.map(lambda examples: tokenizer(examples['text'], truncation=True, padding=True), batched=True)\n",
        "language_to_keep = ['COBOL','Go','C','C++','Java','Python','Perl','Kotlin','C#','Rust','Ruby',\n",
        "'Fortran','JavaScript','Scala','ARM Assembly','Lua','Pascal','jq','AppleScript','Mathematica/Wolfram Language','Swift','Erlang','R','PowerShell',\n",
        "         'Visual Basic .NET','PHP']"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1704451222052
        }
      },
      "id": "82e78251-9fab-4aea-a3e8-86af4305bc69"
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_datasets['input_ids']\n",
        "tokenized_datasets = tokenized_datasets.filter(lambda example: example['label'] in language_to_keep)\n",
        "tokenized_datasets"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "Dataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 26990\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1704451222228
        }
      },
      "id": "3cd7247e-36c6-4b43-ab24-12992dec1ae8"
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = set(data[\"label\"])\n",
        "# # print(labels)\n",
        "# label2id, id2label = dict(), dict()\n",
        "# for i, label in enumerate(labels):\n",
        "#     label2id[label] = str(i)\n",
        "#     id2label[str(i)] = label"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1704451222488
        }
      },
      "id": "d4a88acd-4118-4591-9d74-884beac33eb2"
    },
    {
      "cell_type": "code",
      "source": [
        "# 'H' in labels"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1704451222646
        }
      },
      "id": "923e000f-db84-4e54-b198-b33a02182649"
    },
    {
      "cell_type": "code",
      "source": [
        "# set(data['language_name'])\n",
        "data,tokenized_datasets"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "(Dataset({\n     features: ['text', 'label'],\n     num_rows: 66148\n }),\n Dataset({\n     features: ['text', 'label', 'input_ids', 'attention_mask'],\n     num_rows: 26990\n }))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1704451222831
        }
      },
      "id": "4172ceb4-8040-4b93-878d-0bfddd09745b"
    },
    {
      "cell_type": "code",
      "source": [
        "# data[0]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1704451223006
        }
      },
      "id": "5b3187b3-f77a-4644-95f3-8c2f5d43c233"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "label_mapping = {label: i for i, label in enumerate(set(tokenized_datasets['label']))}\n",
        "# print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "def map_labels(examples):\n",
        "    if 'label' in examples:\n",
        "        return {'label':label_mapping[examples['label']]}\n",
        "    else:\n",
        "        return {'label': []}  # or any other appropriate handling\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.map(map_labels)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/26990 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f65d42cd718548b6a6081c5bf6645831"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1704451228153
        }
      },
      "id": "ff03eac6-baa4-4c8d-a817-a785315658a7"
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "{'Scala': 0,\n 'JavaScript': 1,\n 'COBOL': 2,\n 'ARM Assembly': 3,\n 'R': 4,\n 'Lua': 5,\n 'C++': 6,\n 'Visual Basic .NET': 7,\n 'Go': 8,\n 'Erlang': 9,\n 'C#': 10,\n 'Rust': 11,\n 'Ruby': 12,\n 'Swift': 13,\n 'Mathematica/Wolfram Language': 14,\n 'PHP': 15,\n 'Fortran': 16,\n 'AppleScript': 17,\n 'Pascal': 18,\n 'Java': 19,\n 'PowerShell': 20,\n 'Python': 21,\n 'C': 22,\n 'Perl': 23,\n 'Kotlin': 24,\n 'jq': 25}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704451228329
        }
      },
      "id": "17c1e474-964b-4030-964e-cb6195be0ce9"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[0]['label'],label_mapping['AppleScript']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "(14, 17)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1704451228484
        }
      },
      "id": "6bbad615-6607-4b8c-8b2c-78a3add38354"
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {y: x for x, y in label_mapping.items()}"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1704451228627
        }
      },
      "id": "69ed37b2-d1ce-47b7-b233-ca1a21e0e16f"
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install scikit-learn"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704451228761
        }
      },
      "id": "a18e0f0a-77a9-43cc-97ef-1004dc74ec7f"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1704451230122
        }
      },
      "id": "8dc06567-fd21-413b-b23c-3f6120d87ca6"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=len(set(tokenized_datasets['label'])), id2label=id2label, label2id=label_mapping,ignore_mismatched_sizes=True\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1704451230433
        }
      },
      "id": "c665a5db-4644-44b6-80fc-bd7e118b1120"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704446922783
        }
      },
      "id": "8753e7bd-d2ec-465c-a76f-1569476ee1cd"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tokenized_datasets_splited = tokenized_datasets.train_test_split(test_size=0.2, seed=42)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1704451230563
        }
      },
      "id": "dd089b5a-caa0-4213-9229-2e4b23dd7f6c"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets_splited"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 21592\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5398\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1704451230942
        }
      },
      "id": "686484d6-ac84-4b00-a171-4c421b9f28c8"
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets_splited[\"train\"],\n",
        "    eval_dataset=tokenized_datasets_splited[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='16875' max='16875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16875/16875 7:00:14, Epoch 25/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.831300</td>\n      <td>0.240185</td>\n      <td>0.944053</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.237200</td>\n      <td>0.211322</td>\n      <td>0.950722</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.129300</td>\n      <td>0.206144</td>\n      <td>0.953501</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.086000</td>\n      <td>0.214162</td>\n      <td>0.951834</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.074200</td>\n      <td>0.231554</td>\n      <td>0.953316</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.058100</td>\n      <td>0.241408</td>\n      <td>0.952946</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.046100</td>\n      <td>0.240300</td>\n      <td>0.952205</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.044100</td>\n      <td>0.247937</td>\n      <td>0.952390</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.038200</td>\n      <td>0.256519</td>\n      <td>0.953316</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.038600</td>\n      <td>0.254281</td>\n      <td>0.950167</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.036800</td>\n      <td>0.271672</td>\n      <td>0.951278</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.033900</td>\n      <td>0.258964</td>\n      <td>0.951464</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.033900</td>\n      <td>0.270247</td>\n      <td>0.948870</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.029600</td>\n      <td>0.270773</td>\n      <td>0.952390</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.027000</td>\n      <td>0.272322</td>\n      <td>0.950167</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.027900</td>\n      <td>0.275359</td>\n      <td>0.950167</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.025900</td>\n      <td>0.280984</td>\n      <td>0.949796</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.023900</td>\n      <td>0.279730</td>\n      <td>0.950167</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.022300</td>\n      <td>0.280670</td>\n      <td>0.950352</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.022600</td>\n      <td>0.281365</td>\n      <td>0.951278</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.021200</td>\n      <td>0.286125</td>\n      <td>0.951093</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.018400</td>\n      <td>0.289295</td>\n      <td>0.951464</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.019600</td>\n      <td>0.288483</td>\n      <td>0.950537</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.018900</td>\n      <td>0.291200</td>\n      <td>0.950722</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.017500</td>\n      <td>0.292214</td>\n      <td>0.950722</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric loss:\n0.8313\nAttempted to log scalar metric learning_rate:\n1.9407407407407407e-05\nAttempted to log scalar metric epoch:\n0.74\nAttempted to log scalar metric eval_loss:\n0.24018527567386627\nAttempted to log scalar metric eval_accuracy:\n0.9440533530937384\nAttempted to log scalar metric eval_runtime:\n81.2931\nAttempted to log scalar metric eval_samples_per_second:\n66.402\nAttempted to log scalar metric eval_steps_per_second:\n2.079\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric loss:\n0.2372\nAttempted to log scalar metric learning_rate:\n1.8814814814814816e-05\nAttempted to log scalar metric epoch:\n1.48\nAttempted to log scalar metric eval_loss:\n0.21132202446460724\nAttempted to log scalar metric eval_accuracy:\n0.9507224898110411\nAttempted to log scalar metric eval_runtime:\n81.2253\nAttempted to log scalar metric eval_samples_per_second:\n66.457\nAttempted to log scalar metric eval_steps_per_second:\n2.081\nAttempted to log scalar metric epoch:\n2.0\nAttempted to log scalar metric loss:\n0.1545\nAttempted to log scalar metric learning_rate:\n1.8222222222222224e-05\nAttempted to log scalar metric epoch:\n2.22\nAttempted to log scalar metric loss:\n0.1293\nAttempted to log scalar metric learning_rate:\n1.7629629629629633e-05\nAttempted to log scalar metric epoch:\n2.96\nAttempted to log scalar metric eval_loss:\n0.20614442229270935\nAttempted to log scalar metric eval_accuracy:\n0.9535012967765839\nAttempted to log scalar metric eval_runtime:\n81.141\nAttempted to log scalar metric eval_samples_per_second:\n66.526\nAttempted to log scalar metric eval_steps_per_second:\n2.083\nAttempted to log scalar metric epoch:\n3.0\nAttempted to log scalar metric loss:\n0.086\nAttempted to log scalar metric learning_rate:\n1.7037037037037038e-05\nAttempted to log scalar metric epoch:\n3.7\nAttempted to log scalar metric eval_loss:\n0.21416166424751282\nAttempted to log scalar metric eval_accuracy:\n0.9518340125972582\nAttempted to log scalar metric eval_runtime:\n81.2824\nAttempted to log scalar metric eval_samples_per_second:\n66.41\nAttempted to log scalar metric eval_steps_per_second:\n2.079\nAttempted to log scalar metric epoch:\n4.0\nAttempted to log scalar metric loss:\n0.0742\nAttempted to log scalar metric learning_rate:\n1.6444444444444444e-05\nAttempted to log scalar metric epoch:\n4.44\nAttempted to log scalar metric eval_loss:\n0.2315535843372345\nAttempted to log scalar metric eval_accuracy:\n0.9533160429788811\nAttempted to log scalar metric eval_runtime:\n81.3853\nAttempted to log scalar metric eval_samples_per_second:\n66.326\nAttempted to log scalar metric eval_steps_per_second:\n2.077\nAttempted to log scalar metric epoch:\n5.0\nAttempted to log scalar metric loss:\n0.0611\nAttempted to log scalar metric learning_rate:\n1.5851851851851852e-05\nAttempted to log scalar metric epoch:\n5.19\nAttempted to log scalar metric loss:\n0.0581\nAttempted to log scalar metric learning_rate:\n1.525925925925926e-05\nAttempted to log scalar metric epoch:\n5.93\nAttempted to log scalar metric eval_loss:\n0.2414083331823349\nAttempted to log scalar metric eval_accuracy:\n0.9529455353834754\nAttempted to log scalar metric eval_runtime:\n81.4627\nAttempted to log scalar metric eval_samples_per_second:\n66.263\nAttempted to log scalar metric eval_steps_per_second:\n2.075\nAttempted to log scalar metric epoch:\n6.0\nAttempted to log scalar metric loss:\n0.0461\nAttempted to log scalar metric learning_rate:\n1.4666666666666666e-05\nAttempted to log scalar metric epoch:\n6.67\nAttempted to log scalar metric eval_loss:\n0.24029961228370667\nAttempted to log scalar metric eval_accuracy:\n0.952204520192664\nAttempted to log scalar metric eval_runtime:\n81.3482\nAttempted to log scalar metric eval_samples_per_second:\n66.357\nAttempted to log scalar metric eval_steps_per_second:\n2.077\nAttempted to log scalar metric epoch:\n7.0\nAttempted to log scalar metric loss:\n0.0441\nAttempted to log scalar metric learning_rate:\n1.4074074074074075e-05\nAttempted to log scalar metric epoch:\n7.41\nAttempted to log scalar metric eval_loss:\n0.24793674051761627\nAttempted to log scalar metric eval_accuracy:\n0.9523897739903668\nAttempted to log scalar metric eval_runtime:\n81.3018\nAttempted to log scalar metric eval_samples_per_second:\n66.395\nAttempted to log scalar metric eval_steps_per_second:\n2.079\nAttempted to log scalar metric epoch:\n8.0\nAttempted to log scalar metric loss:\n0.0444\nAttempted to log scalar metric learning_rate:\n1.3481481481481482e-05\nAttempted to log scalar metric epoch:\n8.15\nAttempted to log scalar metric loss:\n0.0382\nAttempted to log scalar metric learning_rate:\n1.288888888888889e-05\nAttempted to log scalar metric epoch:\n8.89\nAttempted to log scalar metric eval_loss:\n0.2565191686153412\nAttempted to log scalar metric eval_accuracy:\n0.9533160429788811\nAttempted to log scalar metric eval_runtime:\n81.4181\nAttempted to log scalar metric eval_samples_per_second:\n66.3\nAttempted to log scalar metric eval_steps_per_second:\n2.076\nAttempted to log scalar metric epoch:\n9.0\nAttempted to log scalar metric loss:\n0.0386\nAttempted to log scalar metric learning_rate:\n1.2296296296296298e-05\nAttempted to log scalar metric epoch:\n9.63\nAttempted to log scalar metric eval_loss:\n0.25428077578544617\nAttempted to log scalar metric eval_accuracy:\n0.9501667284179326\nAttempted to log scalar metric eval_runtime:\n81.2755\nAttempted to log scalar metric eval_samples_per_second:\n66.416\nAttempted to log scalar metric eval_steps_per_second:\n2.079\nAttempted to log scalar metric epoch:\n10.0\nAttempted to log scalar metric loss:\n0.0368\nAttempted to log scalar metric learning_rate:\n1.1703703703703703e-05\nAttempted to log scalar metric epoch:\n10.37\nAttempted to log scalar metric eval_loss:\n0.27167215943336487\nAttempted to log scalar metric eval_accuracy:\n0.9512782512041497\nAttempted to log scalar metric eval_runtime:\n81.2796\nAttempted to log scalar metric eval_samples_per_second:\n66.413\nAttempted to log scalar metric eval_steps_per_second:\n2.079\nAttempted to log scalar metric epoch:\n11.0\nAttempted to log scalar metric loss:\n0.0334\nAttempted to log scalar metric learning_rate:\n1.1111111111111113e-05\nAttempted to log scalar metric epoch:\n11.11\nAttempted to log scalar metric loss:\n0.0339\nAttempted to log scalar metric learning_rate:\n1.0518518518518519e-05\nAttempted to log scalar metric epoch:\n11.85\nAttempted to log scalar metric eval_loss:\n0.25896406173706055\nAttempted to log scalar metric eval_accuracy:\n0.9514635050018525\nAttempted to log scalar metric eval_runtime:\n81.2801\nAttempted to log scalar metric eval_samples_per_second:\n66.412\nAttempted to log scalar metric eval_steps_per_second:\n2.079\nAttempted to log scalar metric epoch:\n12.0\nAttempted to log scalar metric loss:\n0.0339\nAttempted to log scalar metric learning_rate:\n9.925925925925927e-06\nAttempted to log scalar metric epoch:\n12.59\nAttempted to log scalar metric eval_loss:\n0.270246684551239\nAttempted to log scalar metric eval_accuracy:\n0.9488699518340126\nAttempted to log scalar metric eval_runtime:\n81.4251\nAttempted to log scalar metric eval_samples_per_second:\n66.294\nAttempted to log scalar metric eval_steps_per_second:\n2.076\nAttempted to log scalar metric epoch:\n13.0\nAttempted to log scalar metric loss:\n0.0296\nAttempted to log scalar metric learning_rate:\n9.333333333333334e-06\nAttempted to log scalar metric epoch:\n13.33\nAttempted to log scalar metric eval_loss:\n0.27077293395996094\nAttempted to log scalar metric eval_accuracy:\n0.9523897739903668\nAttempted to log scalar metric eval_runtime:\n81.4205\nAttempted to log scalar metric eval_samples_per_second:\n66.298\nAttempted to log scalar metric eval_steps_per_second:\n2.076\nAttempted to log scalar metric epoch:\n14.0\nAttempted to log scalar metric loss:\n0.0298\nAttempted to log scalar metric learning_rate:\n8.740740740740741e-06\nAttempted to log scalar metric epoch:\n14.07\nAttempted to log scalar metric loss:\n0.027\nAttempted to log scalar metric learning_rate:\n8.148148148148148e-06\nAttempted to log scalar metric epoch:\n14.81\nAttempted to log scalar metric eval_loss:\n0.2723221182823181\nAttempted to log scalar metric eval_accuracy:\n0.9501667284179326\nAttempted to log scalar metric eval_runtime:\n81.1965\nAttempted to log scalar metric eval_samples_per_second:\n66.481\nAttempted to log scalar metric eval_steps_per_second:\n2.081\nAttempted to log scalar metric epoch:\n15.0\nAttempted to log scalar metric eval_loss:\n0.27535945177078247\nAttempted to log scalar metric eval_accuracy:\n0.9501667284179326\nAttempted to log scalar metric eval_runtime:\n81.3797\nAttempted to log scalar metric eval_samples_per_second:\n66.331\nAttempted to log scalar metric eval_steps_per_second:\n2.077\nAttempted to log scalar metric epoch:\n16.0\nAttempted to log scalar metric loss:\n0.0259\nAttempted to log scalar metric learning_rate:\n6.962962962962964e-06\nAttempted to log scalar metric epoch:\n16.3\nAttempted to log scalar metric eval_loss:\n0.28098419308662415\nAttempted to log scalar metric eval_accuracy:\n0.9497962208225269\nAttempted to log scalar metric eval_runtime:\n81.1274\nAttempted to log scalar metric eval_samples_per_second:\n66.537\nAttempted to log scalar metric eval_steps_per_second:\n2.083\nAttempted to log scalar metric epoch:\n17.0\nAttempted to log scalar metric loss:\n0.0256\nAttempted to log scalar metric learning_rate:\n6.370370370370371e-06\nAttempted to log scalar metric epoch:\n17.04\nAttempted to log scalar metric loss:\n0.0239\nAttempted to log scalar metric learning_rate:\n5.777777777777778e-06\nAttempted to log scalar metric epoch:\n17.78\nAttempted to log scalar metric eval_loss:\n0.2797295153141022\nAttempted to log scalar metric eval_accuracy:\n0.9501667284179326\nAttempted to log scalar metric eval_runtime:\n81.3175\nAttempted to log scalar metric eval_samples_per_second:\n66.382\nAttempted to log scalar metric eval_steps_per_second:\n2.078\nAttempted to log scalar metric epoch:\n18.0\nAttempted to log scalar metric loss:\n0.0223\nAttempted to log scalar metric learning_rate:\n5.185185185185185e-06\nAttempted to log scalar metric epoch:\n18.52\nAttempted to log scalar metric eval_loss:\n0.28067025542259216\nAttempted to log scalar metric eval_accuracy:\n0.9503519822156354\nAttempted to log scalar metric eval_runtime:\n81.5845\nAttempted to log scalar metric eval_samples_per_second:\n66.165\nAttempted to log scalar metric eval_steps_per_second:\n2.071\nAttempted to log scalar metric epoch:\n19.0\nAttempted to log scalar metric loss:\n0.0231\nAttempted to log scalar metric learning_rate:\n4.592592592592593e-06\nAttempted to log scalar metric epoch:\n19.26\nAttempted to log scalar metric loss:\n0.0226\nAttempted to log scalar metric learning_rate:\n4.000000000000001e-06\nAttempted to log scalar metric epoch:\n20.0\nAttempted to log scalar metric eval_loss:\n0.2813650667667389\nAttempted to log scalar metric eval_accuracy:\n0.9512782512041497\nAttempted to log scalar metric eval_runtime:\n81.5994\nAttempted to log scalar metric eval_samples_per_second:\n66.152\nAttempted to log scalar metric eval_steps_per_second:\n2.071\nAttempted to log scalar metric epoch:\n20.0\nAttempted to log scalar metric loss:\n0.0212\nAttempted to log scalar metric learning_rate:\n3.4074074074074077e-06\nAttempted to log scalar metric epoch:\n20.74\nAttempted to log scalar metric eval_loss:\n0.28612494468688965\nAttempted to log scalar metric eval_accuracy:\n0.9510929974064468\nAttempted to log scalar metric eval_runtime:\n81.6672\nAttempted to log scalar metric eval_samples_per_second:\n66.098\nAttempted to log scalar metric eval_steps_per_second:\n2.069\nAttempted to log scalar metric epoch:\n21.0\nAttempted to log scalar metric loss:\n0.0184\nAttempted to log scalar metric learning_rate:\n2.814814814814815e-06\nAttempted to log scalar metric epoch:\n21.48\nAttempted to log scalar metric eval_loss:\n0.2892947494983673\nAttempted to log scalar metric eval_accuracy:\n0.9514635050018525\nAttempted to log scalar metric eval_runtime:\n81.3224\nAttempted to log scalar metric eval_samples_per_second:\n66.378\nAttempted to log scalar metric eval_steps_per_second:\n2.078\nAttempted to log scalar metric epoch:\n22.0\nAttempted to log scalar metric loss:\n0.0228\nAttempted to log scalar metric learning_rate:\n2.222222222222222e-06\nAttempted to log scalar metric epoch:\n22.22\nAttempted to log scalar metric loss:\n0.0196\nAttempted to log scalar metric learning_rate:\n1.62962962962963e-06\nAttempted to log scalar metric epoch:\n22.96\nAttempted to log scalar metric eval_loss:\n0.288483202457428\nAttempted to log scalar metric eval_accuracy:\n0.9505372360133383\nAttempted to log scalar metric eval_runtime:\n84.5497\nAttempted to log scalar metric eval_samples_per_second:\n63.844\nAttempted to log scalar metric eval_steps_per_second:\n1.999\nAttempted to log scalar metric epoch:\n23.0\nAttempted to log scalar metric loss:\n0.0189\nAttempted to log scalar metric learning_rate:\n1.0370370370370371e-06\nAttempted to log scalar metric epoch:\n23.7\nAttempted to log scalar metric eval_loss:\n0.2911998927593231\nAttempted to log scalar metric eval_accuracy:\n0.9507224898110411\nAttempted to log scalar metric eval_runtime:\n81.4398\nAttempted to log scalar metric eval_samples_per_second:\n66.282\nAttempted to log scalar metric eval_steps_per_second:\n2.075\nAttempted to log scalar metric epoch:\n24.0\nAttempted to log scalar metric loss:\n0.0175\nAttempted to log scalar metric learning_rate:\n4.444444444444445e-07\nAttempted to log scalar metric epoch:\n24.44\nAttempted to log scalar metric eval_loss:\n0.2922140955924988\nAttempted to log scalar metric eval_accuracy:\n0.9507224898110411\nAttempted to log scalar metric eval_runtime:\n81.4673\nAttempted to log scalar metric eval_samples_per_second:\n66.26\nAttempted to log scalar metric eval_steps_per_second:\n2.074\nAttempted to log scalar metric epoch:\n25.0\nAttempted to log scalar metric train_runtime:\n25216.2087\nAttempted to log scalar metric train_samples_per_second:\n21.407\nAttempted to log scalar metric train_steps_per_second:\n0.669\nAttempted to log scalar metric total_flos:\n7.15365067444224e+16\nAttempted to log scalar metric train_loss:\n0.0703159267284252\nAttempted to log scalar metric epoch:\n25.0\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Checkpoint destination directory my_awesome_model/checkpoint-675 already exists and is non-empty.Saving will proceed but saved results may be invalid.\nCheckpoint destination directory my_awesome_model/checkpoint-1350 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "TrainOutput(global_step=16875, training_loss=0.0703159267284252, metrics={'train_runtime': 25216.2087, 'train_samples_per_second': 21.407, 'train_steps_per_second': 0.669, 'total_flos': 7.15365067444224e+16, 'train_loss': 0.0703159267284252, 'epoch': 25.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1704476447446
        }
      },
      "id": "110eee2f-823b-447d-9c11-7a2657cb6768"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./trained_model/\")"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704476449867
        }
      },
      "id": "91e42bb7-d98f-4b41-a844-f0d34cca430a"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: transformers[torch] in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (4.36.2)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (23.0)\nRequirement already satisfied: safetensors>=0.3.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (0.4.1)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (0.20.1)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (0.15.0)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: requests in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: filelock in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (2.1.2+cu118)\nRequirement already satisfied: accelerate>=0.21.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from transformers[torch]) (0.25.0)\nRequirement already satisfied: psutil in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.6.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.10.0)\nRequirement already satisfied: triton==2.1.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\nRequirement already satisfied: sympy in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests->transformers[torch]) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests->transformers[torch]) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from requests->transformers[torch]) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.0.1)\nRequirement already satisfied: mpmath>=0.19 in /anaconda/envs/jupyter_env/lib/python3.8/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1704476452753
        }
      },
      "id": "f7634e25-632b-4d60-9854-188dfd52482a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Code for the script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "68bfdcc0-6488-408c-8f71-c7163ffa7595"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "model_name = './trained_model/'\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704476457285
        }
      },
      "id": "e16d2ce0-7414-45fc-b6e0-39e0ef06c0e2"
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.config\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"./trained_model/\",\n  \"architectures\": [\n    \"RobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"Scala\",\n    \"1\": \"JavaScript\",\n    \"2\": \"COBOL\",\n    \"3\": \"ARM Assembly\",\n    \"4\": \"R\",\n    \"5\": \"Lua\",\n    \"6\": \"C++\",\n    \"7\": \"Visual Basic .NET\",\n    \"8\": \"Go\",\n    \"9\": \"Erlang\",\n    \"10\": \"C#\",\n    \"11\": \"Rust\",\n    \"12\": \"Ruby\",\n    \"13\": \"Swift\",\n    \"14\": \"Mathematica/Wolfram Language\",\n    \"15\": \"PHP\",\n    \"16\": \"Fortran\",\n    \"17\": \"AppleScript\",\n    \"18\": \"Pascal\",\n    \"19\": \"Java\",\n    \"20\": \"PowerShell\",\n    \"21\": \"Python\",\n    \"22\": \"C\",\n    \"23\": \"Perl\",\n    \"24\": \"Kotlin\",\n    \"25\": \"jq\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"ARM Assembly\": 3,\n    \"AppleScript\": 17,\n    \"C\": 22,\n    \"C#\": 10,\n    \"C++\": 6,\n    \"COBOL\": 2,\n    \"Erlang\": 9,\n    \"Fortran\": 16,\n    \"Go\": 8,\n    \"Java\": 19,\n    \"JavaScript\": 1,\n    \"Kotlin\": 24,\n    \"Lua\": 5,\n    \"Mathematica/Wolfram Language\": 14,\n    \"PHP\": 15,\n    \"Pascal\": 18,\n    \"Perl\": 23,\n    \"PowerShell\": 20,\n    \"Python\": 21,\n    \"R\": 4,\n    \"Ruby\": 12,\n    \"Rust\": 11,\n    \"Scala\": 0,\n    \"Swift\": 13,\n    \"Visual Basic .NET\": 7,\n    \"jq\": 25\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"problem_type\": \"single_label_classification\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.36.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 52000\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704477150939
        }
      },
      "id": "4be586cd-b1c0-4870-802b-fcf22a10524d"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "text = \"\"\"\n",
        "tell application \"Slack\" to activate\n",
        "tell application \"Slack\" to quit\n",
        "\n",
        "\"\"\"\n",
        "inputs = loaded_tokenizer(text, return_tensors=\"pt\",truncation=True)\n",
        "# inputs.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = loaded_model(**inputs).logits\n",
        "predicted_class_id = logits.argmax().item()\n",
        "loaded_model.config.id2label[predicted_class_id]\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "'AppleScript'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1704477160839
        }
      },
      "id": "ca1f33b8-3a19-4bc3-8150-d7eec7380e58"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "75bda386-2dc6-4149-8e92-0d53a13703f8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}